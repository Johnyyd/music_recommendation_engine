# ğŸš€ HÆ°á»›ng dáº«n Cáº¥u hÃ¬nh MÃY 1 (Master Node)

Sá»­ dá»¥ng file nÃ y **SAU KHI** MÃ¡y 2 Ä‘Ã£ hoÃ n táº¥t cÃ i Ä‘áº·t theo file `SETUP_WORKER.md`.

## BÆ°á»›c 1: Thiáº¿t láº­p SSH khÃ´ng máº­t kháº©u

MÃ¡y 1 (mÃ¡y cá»§a báº¡n) cáº§n cÃ³ kháº£ nÄƒng Ä‘Äƒng nháº­p vÃ o MÃ¡y 2 Ä‘á»ƒ khá»Ÿi Ä‘á»™ng cÃ¡c dá»‹ch vá»¥.

1.  **Táº¡o Key (Náº¿u chÆ°a cÃ³):**
    ```bash
    ssh-keygen -t rsa
    ```

2.  **Sao chÃ©p Key sang MÃ¡y 2 (Khuyáº¿n nghá»‹):**
    * ÄÃ¢y lÃ  cÃ¡ch tá»± Ä‘á»™ng vÃ  an toÃ n nháº¥t. NÃ³ sáº½ yÃªu cáº§u máº­t kháº©u cá»§a MÃ¡y 2 láº§n cuá»‘i cÃ¹ng.
    * Thay `user_may_2` vÃ  `<IP_MAY_2>`.

    ```bash
    ssh-copy-id user_may_2@<IP_MAY_2>
    ```

3.  **Kiá»ƒm tra:**
    * Thá»­ Ä‘Äƒng nháº­p vÃ o MÃ¡y 2. Báº¡n pháº£i vÃ o Ä‘Æ°á»£c tháº³ng mÃ  khÃ´ng cáº§n há»i máº­t kháº©u.
    ```bash
    ssh user_may_2@<IP_MAY_2>
    ```

## BÆ°á»›c 2: Cáº­p nháº­t danh sÃ¡ch Worker

BÃ¡o cho Hadoop vÃ  Spark biáº¿t vá» worker má»›i (MÃ¡y 2).

1.  **Cáº­p nháº­t Hadoop Workers:**
    * Má»Ÿ file: `$HADOOP_HOME/etc/hadoop/workers`
    * XÃ³a `localhost` (náº¿u cÃ³) vÃ  Ä‘áº£m báº£o file cÃ³ ná»™i dung:
    ```
    172.19.67.26
    <IP_MAY_2>
    ```

2.  **Cáº­p nháº­t Spark Workers:**
    * Má»Ÿ file: `$SPARK_HOME/conf/workers`
    * Äáº£m báº£o file nÃ y cÃ³ ná»™i dung y há»‡t file trÃªn:
    ```
    172.19.67.26
    <IP_MAY_2>
    ```

## BÆ°á»›c 3: Khá»Ÿi Ä‘á»™ng láº¡i ToÃ n bá»™ Cá»¥m

Thá»±c hiá»‡n tá»« **MÃ¡y 1** (mÃ¡y cá»§a báº¡n).

1.  **Dá»«ng táº¥t cáº£ dá»‹ch vá»¥ (náº¿u Ä‘ang cháº¡y):**
    ```bash
    $SPARK_HOME/sbin/stop-all.sh
    $HADOOP_HOME/sbin/stop-dfs.sh
    ```

2.  **Khá»Ÿi Ä‘á»™ng HDFS:**
    ```bash
    $HADOOP_HOME/sbin/start-dfs.sh
    ```
    *(Báº¡n sáº½ tháº¥y log bÃ¡o khá»Ÿi Ä‘á»™ng NameNode/DataNode trÃªn mÃ¡y nÃ y vÃ  DataNode trÃªn MÃ¡y 2).*

3.  **Khá»Ÿi Ä‘á»™ng Spark:**
    ```bash
    $SPARK_HOME/sbin/start-all.sh
    ```
    *(Báº¡n sáº½ tháº¥y log bÃ¡o khá»Ÿi Ä‘á»™ng Master/Worker trÃªn mÃ¡y nÃ y vÃ  Worker trÃªn MÃ¡y 2).*

## BÆ°á»›c 4: Kiá»ƒm tra Tráº¡ng thÃ¡i Cá»¥m

1.  **Kiá»ƒm tra HDFS:**
    * Má»Ÿ trÃ¬nh duyá»‡t: `http://172.19.67.26:9870`
    * VÃ o tab **"Datanodes"**. Báº¡n pháº£i tháº¥y **2 Datanodes** Ä‘ang hoáº¡t Ä‘á»™ng (Live).

2.  **Kiá»ƒm tra Spark:**
    * Má»Ÿ trÃ¬nh duyá»‡t: `http://172.19.67.26:8080`
    * Báº¡n pháº£i tháº¥y **Alive Workers: 2**.

## BÆ°á»›c 5: Cháº¡y Pipeline

Náº¿u cáº£ hai bÆ°á»›c kiá»ƒm tra trÃªn Ä‘á»u thÃ nh cÃ´ng, cá»¥m cá»§a báº¡n Ä‘Ã£ sáºµn sÃ ng.

Chá»‰ cáº§n cháº¡y pipeline nhÆ° bÃ¬nh thÆ°á»ng tá»« MÃ¡y 1. Spark Master sáº½ tá»± Ä‘á»™ng phÃ¢n chia cÃ´ng viá»‡c (bÆ°á»›c 4/5 vÃ  5/5) cho cáº£ hai mÃ¡y worker.

```bash
./run_pipeline.sh
```