#!/bin/bash
set -e  # ThoÃ¡t náº¿u cÃ³ lá»—i

# Cáº¥u hÃ¬nh
node="172.19.67.26" # Sá»¬ Dá»¤NG IP MASTER Ä‘á»ƒ truy cáº­p HDFS
master_url="spark://172.19.67.26:7077" # Sá»¬ Dá»¤NG IP MASTER Ä‘á»ƒ Master láº¯ng nghe
timestamp=$(date +%Y%m%d_%H%M%S)
log_dir="logs"
log_file="${log_dir}/pipeline_${timestamp}.log"

# Tham sá»‘ Executor (Tá»‘i Æ°u hÃ³a cho 2 node)
NUM_EXECUTORS=2        
EXECUTOR_CORES=2       
EXECUTOR_MEMORY="3g"   
DRIVER_MEMORY="5g"     
SHUFFLE_PARTITIONS=64

# ========================================
# KIá»‚M TRA & Táº O THÆ¯ Má»¤C
# ========================================

# Táº¡o thÆ° má»¥c logs cá»¥c bá»™
mkdir -p $log_dir

# Äáº£m báº£o thÆ° má»¥c táº¡m cho Spark tá»“n táº¡i trÃªn cá»¥c bá»™
mkdir -p /tmp/spark || true

# Táº¡o táº¥t cáº£ cÃ¡c thÆ° má»¥c HDFS cáº§n thiáº¿t cho dá»¯ liá»‡u, model, vÃ  checkpoints
log "--- Äang kiá»ƒm tra vÃ  táº¡o cÃ¡c thÆ° má»¥c HDFS cáº§n thiáº¿t ---"
hdfs dfs -mkdir -p /data/mqd/raw || true
hdfs dfs -mkdir -p /data/mqd/parquet || true
hdfs dfs -mkdir -p /data/mqd/meta/playlist_indexer || true
hdfs dfs -mkdir -p /data/mqd/meta/track_indexer || true
hdfs dfs -mkdir -p /data/mqd/train || true
hdfs dfs -mkdir -p /data/mqd/test || true
hdfs dfs -mkdir -p /data/mqd/backup || true
hdfs dfs -mkdir -p /model/backup || true
hdfs dfs -mkdir -p /tmp/spark_checkpoints || true

# ========================================
# Báº®T Äáº¦U PIPELINE
# ========================================

# HÃ m log
log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1" | tee -a "$log_file"
}

# HÃ m kiá»ƒm tra exit code
check_status() {
    if [ $? -ne 0 ]; then
        log "âŒ ERROR: Step $1 failed"
        exit 1
    fi
}

# Backup dá»¯ liá»‡u hiá»‡n cÃ³
log "=== [0/5] Backing up existing data ==="
# LÆ°u Ã½: Lá»‡nh nÃ y Ä‘Ã£ bao gá»“m kiá»ƒm tra thÆ° má»¥c.
hdfs dfs -test -d /data/mqd/parquet && {
    backup_dir="/data/mqd/backup/parquet_${timestamp}"
    hdfs dfs -mv /data/mqd/parquet $backup_dir
    log "âœ“ Backed up parquet data to $backup_dir"
} || log "No existing parquet data to backup"
check_status "Backup"

# Pipeline chÃ­nh
log "=== [1/5] ETL JSON â†’ Parquet ==="
HDFS_NODE="$node" spark-submit \
  --master "$master_url" \
  --conf spark.sql.shuffle.partitions=$SHUFFLE_PARTITIONS \
  /mnt/c/LUUDULIEU/CODE/github/music_recommendation_engine/etl_to_parquet.py
check_status "ETL"

log "=== [2/5] Split Train/Test Data ==="
HDFS_NODE="$node" spark-submit \
  --master "$master_url" \
  --conf spark.sql.shuffle.partitions=$SHUFFLE_PARTITIONS \
  /mnt/c/LUUDULIEU/CODE/github/music_recommendation_engine/split_train_test.py
check_status "Data Split"

log "=== [3/5] Train ALS Model ==="
HDFS_NODE="$node" spark-submit \
  --master "$master_url" \
  --num-executors $NUM_EXECUTORS \
  --executor-cores $EXECUTOR_CORES \
  --executor-memory $EXECUTOR_MEMORY \
  --driver-memory $DRIVER_MEMORY \
  --conf spark.driver.maxResultSize="2g" \
  --conf spark.sql.shuffle.partitions=64 \
  --conf spark.default.parallelism=64 \
  --conf spark.local.dir=/tmp/spark \
  /mnt/c/LUUDULIEU/CODE/github/music_recommendation_engine/train_als_model.py
check_status "Training"

log "=== [4/5] Evaluate Model ==="
HDFS_NODE="$node" spark-submit \
  --master "$master_url" \
  --num-executors $NUM_EXECUTORS \
  --executor-cores $EXECUTOR_CORES \
  --executor-memory $EXECUTOR_MEMORY \
  --driver-memory $DRIVER_MEMORY \
  --conf spark.driver.maxResultSize="2g" \
  --conf spark.sql.shuffle.partitions=64 \
  --conf spark.default.parallelism=64 \
  --conf spark.local.dir=/tmp/spark \
  /mnt/c/LUUDULIEU/CODE/github/music_recommendation_engine/evaluation.py
check_status "Evaluation"

log "=== [5/5] Generate Submission ==="
HDFS_NODE="$node" spark-submit \
  --master "$master_url" \
  --num-executors $NUM_EXECUTORS \
  --executor-cores $EXECUTOR_CORES \
  --executor-memory $EXECUTOR_MEMORY \
  --driver-memory $DRIVER_MEMORY \
  --conf spark.driver.maxResultSize="2g" \
  --conf spark.sql.shuffle.partitions=128 \
  --conf spark.default.parallelism=128 \
  --conf spark.local.dir=/tmp/spark \
  /mnt/c/LUUDULIEU/CODE/github/music_recommendation_engine/generate_submission.py
check_status "Submission Generation"

log "âœ… Pipeline completed successfully!"

# LÆ°u metrics vÃ o file
metrics_file="${log_dir}/metrics_history.csv"
# (Giá»¯ nguyÃªn pháº§n lÆ°u metrics)
if [ ! -f "$metrics_file" ]; then
    echo "timestamp,map_score,num_playlists,training_time" > "$metrics_file"
fi

map_score=$(grep "Mean Average Precision @ 500:" "$log_file" | tail -n1 | awk '{print $NF}')
echo "${timestamp},${map_score},$num_playlists,$SECONDS" >> "$metrics_file"

log "ğŸ“Š Metrics saved to ${metrics_file}"