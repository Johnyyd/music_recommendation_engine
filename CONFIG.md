# H∆∞·ªõng d·∫´n C√†i ƒë·∫∑t Cluster HDFS & Spark 2 Node tr√™n WSL (K·∫øt n·ªëi LAN tr·ª±c ti·∫øp)

T√†i li·ªáu n√†y h∆∞·ªõng d·∫´n chi ti·∫øt c√°ch c√†i ƒë·∫∑t m·ªôt cluster (c·ª•m) Hadoop HDFS v√† Spark tr√™n hai m√°y t√≠nh Windows, s·ª≠ d·ª•ng WSL 2 (Ubuntu) cho m·ªói node, v√† k·∫øt n·ªëi ch√∫ng tr·ª±c ti·∫øp b·∫±ng m·ªôt d√¢y LAN (kh√¥ng qua router).

---

## üó∫Ô∏è B·∫£ng k·∫ø ho·∫°ch: T√™n v√† ƒê·ªãa ch·ªâ IP

ƒê·ªÉ tr√°nh nh·∫ßm l·∫´n, ch√∫ng ta th·ªëng nh·∫•t t√™n g·ªçi v√† ƒë·ªãa ch·ªâ IP cho to√†n b·ªô h·ªá th·ªëng:

| M√°y | H·ªá ƒëi·ªÅu h√†nh | T√™n g·ªçi | IP Tƒ©nh (S·∫Ω ƒë·∫∑t ·ªü Ph·∫ßn 1-3) |
| :--- | :--- | :--- | :--- |
| **M√°y 1** | **Windows Host** | `MASTER-HOST` | `192.168.10.1` |
| | **WSL Ubuntu** | `spark-master` | `192.168.10.101` |
| **M√°y 2** | **Windows Host** | `WORKER-HOST` | `192.168.10.2` |
| | **WSL Ubuntu** | `spark-worker` | `192.168.10.102` |

---

## üöß Ph·∫ßn 1: C·∫•u h√¨nh M·∫°ng Windows Host

**M·ª•c ti√™u:** Gi√∫p 2 m√°y t√≠nh Windows "nh√¨n th·∫•y" nhau qua d√¢y LAN.
**Th·ª±c hi·ªán tr√™n:** C·∫£ hai m√°y t√≠nh Windows.

### B∆∞·ªõc 1.1: ƒê·∫∑t IP Tƒ©nh cho Windows

1.  C·∫Øm d√¢y LAN k·∫øt n·ªëi tr·ª±c ti·∫øp 2 m√°y t√≠nh.
2.  **Tr√™n `MASTER-HOST` (M√°y 1 - Windows):**
    * M·ªü **Control Panel** -> **Network and Sharing Center** -> **Change adapter settings**.
    * Chu·ªôt ph·∫£i v√†o card "Ethernet" (ƒëang b√°o "Unidentified network"), ch·ªçn **Properties**.
    * Ch·ªçn **"Internet Protocol Version 4 (TCP/IPv4)"** -> **Properties**.
    * Ch·ªçn **"Use the following IP address"** v√† nh·∫≠p:
        * IP address: `192.168.10.1`
        * Subnet mask: `255.255.255.0`
        * Default gateway: (ƒë·ªÉ tr·ªëng)
    * Nh·∫•p **OK**.

3.  **Tr√™n `WORKER-HOST` (M√°y 2 - Windows):**
    * L√†m y h·ªát M√°y 1, nh∆∞ng nh·∫≠p:
        * IP address: `192.168.10.2`
        * Subnet mask: `255.255.255.0`
        * Default gateway: (ƒë·ªÉ tr·ªëng)
    * Nh·∫•p **OK**.

### B∆∞·ªõc 1.2: Ki·ªÉm tra k·∫øt n·ªëi Windows

* **Tr√™n `MASTER-HOST` (M√°y 1 - Windows):**
* M·ªü **Command Prompt (cmd)**.
* G√µ l·ªánh:
    ```bash
    ping 192.168.10.2
    ```
* N·∫øu b·∫°n th·∫•y `Reply from 192.168.10.2...`, hai m√°y Windows ƒë√£ k·∫øt n·ªëi th√†nh c√¥ng.

---

## üöß Ph·∫ßn 2: C·∫•u h√¨nh M·∫°ng WSL (Ch·∫ø ƒë·ªô C·∫ßu n·ªëi)

**M·ª•c ti√™u:** "B·∫Øc c·∫ßu" cho WSL Ubuntu s·ª≠ d·ª•ng k·∫øt n·ªëi m·∫°ng LAN v·∫≠t l√Ω m√† ch√∫ng ta v·ª´a t·∫°o.
**Th·ª±c hi·ªán tr√™n:** C·∫£ hai m√°y t√≠nh Windows.

### B∆∞·ªõc 2.1: T·∫°o Virtual Switch

1.  **Tr√™n C·∫¢ HAI M√ÅY Windows (`MASTER-HOST` v√† `WORKER-HOST`):**
2.  Nh·∫•n ph√≠m `Windows`, g√µ **"Hyper-V Manager"** v√† m·ªü n√≥. (N·∫øu ch∆∞a c√≥, v√†o "Turn Windows features on or off" ƒë·ªÉ c√†i ƒë·∫∑t).
3.  Trong menu b√™n ph·∫£i, ch·ªçn **"Virtual Switch Manager..."**.
4.  Ch·ªçn **"New virtual network switch"** -> **"External"** -> **"Create Virtual Switch"**.
5.  ƒê·∫∑t t√™n: `WSLBridge`
6.  Trong "External network", ch·ªçn card m·∫°ng **"Ethernet"** v·∫≠t l√Ω (ch√≠nh l√† card b·∫°n v·ª´a ƒë·∫∑t IP tƒ©nh ·ªü Ph·∫ßn 1).
7.  Nh·∫•p **OK**. M·∫°ng c·ªßa b·∫°n c√≥ th·ªÉ b·ªã ng·∫Øt k·∫øt n·ªëi v√†i gi√¢y.

### B∆∞·ªõc 2.2: C·∫•u h√¨nh `.wslconfig`

1.  **Tr√™n C·∫¢ HAI M√ÅY Windows (`MASTER-HOST` v√† `WORKER-HOST`):**
2.  M·ªü File Explorer, g√µ `%UserProfile%` v√†o thanh ƒë·ªãa ch·ªâ v√† nh·∫•n Enter (s·∫Ω m·ªü `C:\Users\<T√™n_C·ªßa_B·∫°n>`).
3.  T·∫°o (ho·∫∑c m·ªü) file `.wslconfig` (kh√¥ng c√≥ t√™n file, ch·ªâ c√≥ ƒëu√¥i).
4.  Copy v√† d√°n n·ªôi dung n√†y v√†o:
    ```ini
    [wsl2]
    vmSwitch = WSLBridge
    ```
5.  **QUAN TR·ªåNG:** M·ªü **Command Prompt (cmd)** v√† g√µ l·ªánh sau ƒë·ªÉ t·∫Øt ho√†n to√†n WSL:
    ```bash
    wsl --shutdown
    ```

---

## üöß Ph·∫ßn 3: C·∫•u h√¨nh IP Tƒ©nh cho WSL Ubuntu

**M·ª•c ti√™u:** ƒê·∫∑t IP tƒ©nh cho 2 m√°y Ubuntu ƒë·ªÉ ch√∫ng c√≥ th·ªÉ li√™n l·∫°c v·ªõi nhau.

### B∆∞·ªõc 3.1: ƒê·∫∑t IP Tƒ©nh

1.  **Tr√™n `spark-master` (M√°y 1 - WSL Ubuntu):**
    * Kh·ªüi ƒë·ªông WSL Ubuntu.
    * G√µ l·ªánh sau (t√™n file c√≥ th·ªÉ l√† `00-eth0.yaml` ho·∫∑c `01-netcfg.yaml`, h√£y d√πng ph√≠m `Tab` ƒë·ªÉ t·ª± ƒë·ªông ƒëi·ªÅn):
        ```bash
        sudo nano /etc/netplan/00-eth0.yaml
        ```
    * X√≥a h·∫øt n·ªôi dung c≈© v√† d√°n n·ªôi dung sau v√†o:
        ```yaml
        network:
          version: 2
          ethernets:
            eth0:
              dhcp4: no
              addresses: [192.168.10.101/24]
              gateway4: 192.168.10.1
              nameservers:
                addresses: [8.8.8.8, 1.1.1.1]
        ```
        *L∆∞u √Ω: `gateway4: 192.168.10.1` tr·ªè ƒë·∫øn IP c·ªßa `MASTER-HOST` (Windows).*

2.  **Tr√™n `spark-worker` (M√°y 2 - WSL Ubuntu):**
    * Kh·ªüi ƒë·ªông WSL Ubuntu.
    * G√µ l·ªánh:
        ```bash
        sudo nano /etc/netplan/00-eth0.yaml
        ```
    * X√≥a h·∫øt n·ªôi dung c≈© v√† d√°n n·ªôi dung sau v√†o:
        ```yaml
        network:
          version: 2
          ethernets:
            eth0:
              dhcp4: no
              addresses: [192.168.10.102/24]
              gateway4: 192.168.10.1
              nameservers:
                addresses: [8.8.8.8, 1.1.1.1]
        ```
        *L∆∞u √Ω: `gateway4` c·ªßa m√°y Worker C≈®NG tr·ªè v·ªÅ IP c·ªßa `MASTER-HOST`.*

3.  **Tr√™n C·∫¢ HAI M√ÅY WSL (`spark-master` v√† `spark-worker`):**
    * √Åp d·ª•ng c·∫•u h√¨nh:
        ```bash
        sudo netplan apply
        ```

---

## üöß Ph·∫ßn 4: T∆∞·ªùng l·ª≠a & Hostnames

**M·ª•c ti√™u:** M·ªü c·ªïng t∆∞·ªùng l·ª≠a v√† gi√∫p c√°c m√°y g·ªçi nhau b·∫±ng t√™n thay v√¨ IP.

### B∆∞·ªõc 4.1: C·∫•u h√¨nh Windows Firewall (R·∫§T QUAN TR·ªåNG)

1.  **Tr√™n C·∫¢ HAI M√ÅY Windows (`MASTER-HOST` v√† `WORKER-HOST`):**
2.  Nh·∫•n `Windows`, g√µ **"Windows Defender Firewall with Advanced Security"** v√† m·ªü n√≥.
3.  Nh·∫•p v√†o **"Inbound Rules"** -> **"New Rule..."** (·ªü menu b√™n ph·∫£i).
4.  Ch·ªçn **"Port"** -> Next.
5.  Ch·ªçn **"TCP"**.
6.  Ch·ªçn **"Specific local ports"** v√† g√µ danh s√°ch c·ªïng sau:
    `22, 7077, 8080, 8081, 9000, 9870, 9866, 8088, 8032`
    (ƒê√¢y l√† c√°c c·ªïng cho SSH, Spark Master, Spark UI, HDFS NameNode, HDFS DataNode, YARN).
7.  Ch·ªçn **"Allow the connection"** -> Next.
8.  **QUAN TR·ªåNG:** ·ªû b∆∞·ªõc "Profile", h√£y **tick ch·ªçn c·∫£ 3 √¥**: "Domain", "Private", v√† **"Public"**. (V√¨ m·∫°ng LAN tr·ª±c ti·∫øp n√†y b·ªã Windows coi l√† "Public"). -> Next.
9.  ƒê·∫∑t t√™n (v√≠ d·ª•: `Spark Cluster Ports`) v√† nh·∫•p **Finish**.

### B∆∞·ªõc 4.2: C·∫•u h√¨nh `/etc/hosts`

1.  **Tr√™n C·∫¢ HAI M√ÅY WSL (`spark-master` v√† `spark-worker`):**
2.  G√µ l·ªánh:
    ```bash
    sudo nano /etc/hosts
    ```
3.  Th√™m 2 d√≤ng sau v√†o cu·ªëi file (s·ª≠ d·ª•ng IP c·ªßa WSL, kh√¥ng ph·∫£i Windows):
    ```
    192.168.10.101   spark-master
    192.168.10.102   spark-worker
    ```

### B∆∞·ªõc 4.3: Ki·ªÉm tra cu·ªëi c√πng

1.  **Tr√™n `spark-master` (M√°y 1 - WSL Ubuntu):**
    ```bash
    ping spark-worker
    ```
2.  **Tr√™n `spark-worker` (M√°y 2 - WSL Ubuntu):**
    ```bash
    ping spark-master
    ```
* N·∫øu c·∫£ hai ƒë·ªÅu `ping` th√†nh c√¥ng (nh·∫≠n ƒë∆∞·ª£c `... bytes from ...`), b·∫°n ƒë√£ ho√†n th√†nh ph·∫ßn m·∫°ng.

---

## üì¶ Ph·∫ßn 5: C√†i ƒë·∫∑t Cluster (Hadoop & Spark)

Gi·ªù ƒë√¢y, b·∫°n c√≥ 2 m√°y Ubuntu tr√™n m·∫°ng LAN, vi·ªác c√†i ƒë·∫∑t s·∫Ω gi·ªëng nh∆∞ m·ªôt cluster b√¨nh th∆∞·ªùng.

### B∆∞·ªõc 5.1: C√†i ƒë·∫∑t Chung (Java, SSH)

1.  **Tr√™n C·∫¢ HAI M√ÅY WSL (`spark-master` v√† `spark-worker`):**
    ```bash
    sudo apt update
    sudo apt install openjdk-11-jdk openssh-server -y
    sudo service ssh start
    ```

### B∆∞·ªõc 5.2: C·∫•u h√¨nh SSH kh√¥ng m·∫≠t kh·∫©u (Master -> Worker)

1.  **Tr√™n `spark-master` (M√°y 1 - WSL Ubuntu):**
    * T·∫°o kh√≥a SSH:
        ```bash
        ssh-keygen -t rsa
        ```
        (Nh·∫•n `Enter` 3 l·∫ßn ƒë·ªÉ ch·∫•p nh·∫≠n m·∫∑c ƒë·ªãnh, kh√¥ng ƒë·∫∑t m·∫≠t kh·∫©u).
    * Copy kh√≥a sang m√°y worker (thay `tringuyen` b·∫±ng t√™n user c·ªßa b·∫°n tr√™n m√°y worker):
        ```bash
        ssh-copy-id tringuyen@spark-worker
        ```
        (Nh·∫≠p m·∫≠t kh·∫©u c·ªßa user `tringuyen` tr√™n m√°y `spark-worker` khi ƒë∆∞·ª£c h·ªèi).

2.  **Ki·ªÉm tra (Tr√™n `spark-master`):**
    ```bash
    ssh spark-worker
    ```
    N·∫øu b·∫°n ƒëƒÉng nh·∫≠p th·∫≥ng v√†o `spark-worker` m√† **kh√¥ng b·ªã h·ªèi m·∫≠t kh·∫©u**, b·∫°n ƒë√£ th√†nh c√¥ng. G√µ `exit` ƒë·ªÉ quay l·∫°i `spark-master`.

### B∆∞·ªõc 5.3: C√†i ƒë·∫∑t Hadoop (HDFS)

1.  **Tr√™n C·∫¢ HAI M√ÅY WSL (`spark-master` v√† `spark-worker`):**
    * T·∫£i Hadoop (v√≠ d·ª• 3.3.6) v√† gi·∫£i n√©n (gi·∫£ s·ª≠ v√†o `/home/tringuyen/hadoop`).
    * Th√™m bi·∫øn m√¥i tr∆∞·ªùng v√†o `.bashrc`:
        ```bash
        nano ~/.bashrc
        ```
    * Th√™m c√°c d√≤ng sau v√†o cu·ªëi file:
        ```bash
        export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
        export HADOOP_HOME=/home/tringuyen/hadoop
        export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
        export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
        ```
    * √Åp d·ª•ng: `source ~/.bashrc`

2.  **Tr√™n `spark-master` (M√°y 1 - WSL Ubuntu):**
    * T·∫°o th∆∞ m·ª•c d·ªØ li·ªáu HDFS:
        ```bash
        mkdir -p /home/tringuyen/hdfs_data/namenode
        mkdir -p /home/tringuyen/hdfs_data/datanode
        ```
    * C·∫•u h√¨nh `hadoop-env.sh`:
        ```bash
        nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh
        ```
        T√¨m d√≤ng `export JAVA_HOME` v√† s·ª≠a th√†nh: `export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64`
    * C·∫•u h√¨nh `core-site.xml`:
        ```bash
        nano $HADOOP_HOME/etc/hadoop/core-site.xml
        ```
        Th√™m v√†o gi·ªØa `<configuration>` v√† `</configuration>`:
        ```xml
        <configuration>
            <property>
                <name>fs.defaultFS</name>
                <value>hdfs://spark-master:9000</value>
            </property>
        </configuration>
        ```
    * C·∫•u h√¨nh `hdfs-site.xml`:
        ```bash
        nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml
        ```
        Th√™m v√†o gi·ªØa `<configuration>` v√† `</configuration>`:
        ```xml
        <configuration>
            <property>
                <name>dfs.replication</name>
                <value>1</value>
            </property>
            <property>
                <name>dfs.namenode.name.dir</name>
                <value>file:///home/tringuyen/hdfs_data/namenode</value>
            </property>
            <property>
                <name>dfs.datanode.data.dir</name>
                <value>file:///home/tringuyen/hdfs_data/datanode</value>
            </property>
        </configuration>
        ```
    * C·∫•u h√¨nh `workers`:
        ```bash
        nano $HADOOP_HOME/etc/hadoop/workers
        ```
        X√≥a `localhost` v√† thay b·∫±ng:
        ```
        spark-worker
        ```

3.  **Copy c·∫•u h√¨nh sang Worker (Tr√™n `spark-master`):**
    ```bash
    scp -r $HADOOP_HOME/etc/hadoop/* spark-worker:$HADOOP_HOME/etc/hadoop/
    ```

4.  **Kh·ªüi ƒë·ªông HDFS (Tr√™n `spark-master`):**
    * Format NameNode (CH·ªà L√ÄM L·∫¶N ƒê·∫¶U):
        ```bash
        hdfs namenode -format
        ```
    * Kh·ªüi ƒë·ªông HDFS:
        ```bash
        start-dfs.sh
        ```

5.  **Ki·ªÉm tra HDFS:**
    * **Tr√™n `spark-master`:** g√µ `jps`. B·∫°n ph·∫£i th·∫•y `NameNode` v√† `SecondaryNameNode`.
    * **Tr√™n `spark-worker`:** g√µ `jps`. B·∫°n ph·∫£i th·∫•y `DataNode`.
    * M·ªü tr√¨nh duy·ªát tr√™n m√°y Windows (v√≠ d·ª• M√°y 1): `http://192.168.10.101:9870`
    * V√†o tab "Datanodes", b·∫°n ph·∫£i th·∫•y "1 Live Nodes".

### B∆∞·ªõc 5.4: C√†i ƒë·∫∑t Spark

1.  **Tr√™n C·∫¢ HAI M√ÅY WSL (`spark-master` v√† `spark-worker`):**
    * T·∫£i Spark (v√≠ d·ª• 3.5.0) v√† gi·∫£i n√©n (gi·∫£ s·ª≠ v√†o `/home/tringuyen/spark`).
    * Th√™m bi·∫øn m√¥i tr∆∞·ªùng v√†o `.bashrc`:
        ```bash
        nano ~/.bashrc
        ```
    * Th√™m v√†o cu·ªëi file:
        ```bash
        export SPARK_HOME=/home/tringuyen/spark
        export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
        ```
    * √Åp d·ª•ng: `source ~/.bashrc`

2.  **Tr√™n `spark-master` (M√°y 1 - WSL Ubuntu):**
    * C·∫•u h√¨nh `spark-env.sh`:
        ```bash
        cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.sh
        nano $SPARK_HOME/conf/spark-env.sh
        ```
        Th√™m c√°c d√≤ng sau v√†o cu·ªëi file:
        ```bash
        export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
        export HADOOP_CONF_DIR=/home/tringuyen/hadoop/etc/hadoop
        export SPARK_MASTER_HOST='spark-master'
        ```
    * C·∫•u h√¨nh `workers`:
        ```bash
        cp $SPARK_HOME/conf/workers.template $SPARK_HOME/conf/workers
        nano $SPARK_HOME/conf/workers
        ```
        X√≥a `localhost` v√† thay b·∫±ng:
        ```
        spark-worker
        ```

3.  **Copy c·∫•u h√¨nh sang Worker (Tr√™n `spark-master`):**
    ```bash
    scp -r $SPARK_HOME/conf/* spark-worker:$SPARK_HOME/conf/
    ```

4.  **Kh·ªüi ƒë·ªông Spark (Tr√™n `spark-master`):**
    ```bash
    start-master.sh
    start-workers.sh
    ```

5.  **Ki·ªÉm tra Spark:**
    * **Tr√™n `spark-master`:** g√µ `jps`. B·∫°n ph·∫£i th·∫•y `Master`.
    * **Tr√™n `spark-worker`:** g√µ `jps`. B·∫°n ph·∫£i th·∫•y `Worker`.
    * M·ªü tr√¨nh duy·ªát tr√™n m√°y Windows (v√≠ d·ª• M√°y 1): `http://192.168.10.101:8080`
    * B·∫°n ph·∫£i th·∫•y 1 "Alive Worker" v·ªõi ƒë·ªãa ch·ªâ `spark-worker`.

---

## üöÄ Ph·∫ßn 6: Ch·∫°y Job c·ªßa b·∫°n

B√¢y gi·ªù b·∫°n ƒë√£ s·∫µn s√†ng!

1.  **Tr√™n `spark-master` (M√°y 1 - WSL Ubuntu):**
2.  M·ªü file `run_pipeline.sh` c·ªßa b·∫°n.
3.  **S·ª≠a 2 ch·ªó quan tr·ªçng:**
    * T√¨m d√≤ng `node="172.19.67.26"` v√† ƒë·ªïi th√†nh `node="spark-master"`.
    * Trong c√°c l·ªánh `spark-submit`, ƒë·ªïi `--master spark://$node:7077` th√†nh `--master spark://spark-master:7077`.
4.  **T·ªëi ∆∞u RAM (V√≠ d·ª•: M√°y 1 c√≥ 8GB, M√°y 2 c√≥ 16GB):**
    * T√¨m ƒë·∫øn b∆∞·ªõc `[4/5] Evaluate Model` v√† s·ª≠a nh∆∞ sau:
        ```bash
        log "=== [4/5] Evaluate Model ==="
        HDFS_NODE="spark-master" spark-submit \
        --master spark://spark-master:7077 \
        --conf spark.driver.memory=6g \
        --conf spark.executor.memory=12g \
        --conf spark.executor.cores=2 \
        ... (gi·ªØ nguy√™n c√°c tham s·ªë kh√°c) ...
        /mnt/c/LUUDULIEU/CODE/github/music_recommendation_engine/evaluation.py
        check_status "Evaluation"
        ```
    * **Gi·∫£i th√≠ch:**
        * `spark.driver.memory=6g`: Driver s·∫Ω ch·∫°y tr√™n `spark-master`, l·∫•y 6GB RAM c·ªßa M√°y 1.
        * `spark.executor.memory=12g`: Executor s·∫Ω ch·∫°y tr√™n `spark-worker`, l·∫•y 12GB RAM c·ªßa M√°y 2.
5.  Ch·∫°y script: `./run_pipeline.sh`
